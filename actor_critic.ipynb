{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow_hub as hub\n",
    "from collections import deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(FReLU, self).__init__()\n",
    "        self.f_cond = tf.keras.layers.DepthwiseConv2D(kernel_size, strides=(1, 1), padding='same')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.f_cond(inputs)\n",
    "        x = self.bn(x)\n",
    "        x_shape = tf.keras.backend.int_shape(x)\n",
    "        x = tf.keras.layers.Lambda(self.max_unit, output_shape=(x_shape[1], x_shape[2], x_shape[3]))([inputs, x])\n",
    "        return x\n",
    "    \n",
    "    def max_unit(self, args):\n",
    "        inputs , depthconv_output = args\n",
    "        return tf.maximum(inputs, depthconv_output)\n",
    "    \n",
    "class Mish(tf.keras.layers.Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, act_spec, shape, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.layer1 = tf.keras.layers.SeparableConv2D(8, 8, strides=4)\n",
    "        self.activation1 = FReLU()\n",
    "        self.layer2 = tf.keras.layers.SeparableConv2D(16, 4, strides=2)\n",
    "        self.activation2 = FReLU()\n",
    "        self.layer3 = tf.keras.layers.SeparableConv2D(32, 3, strides=1)\n",
    "        self.activation3 = FReLU()\n",
    "        self.layer4 = tf.keras.layers.Flatten()\n",
    "        self.lstm = tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    "        self.actor = tf.keras.layers.Dense(act_spec, activation='softmax', name='actor')\n",
    "        self.shape = shape\n",
    "        \n",
    "        self.act_rnn = tf.keras.layers.GRU(64)\n",
    "        self.act_dense = tf.keras.layers.Dense(32)\n",
    "        self.act_activation = Mish(mish)\n",
    "        self.critic = tf.keras.layers.Dense(1, name='critic')\n",
    "        \n",
    "    def call(self, frames, action_seq, state):\n",
    "        x = frames / 255\n",
    "        x = tf.image.resize(x, [self.shape[0], self.shape[1]])\n",
    "        \n",
    "        y = self.act_rnn(action_seq)\n",
    "        y = self.act_dense(y)\n",
    "        y = self.act_activation(y)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        if state is None:\n",
    "            x, m_state, c_state = self.lstm(x)\n",
    "        else:\n",
    "            x, m_state, c_state = self.lstm(x, initial_state=state)\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "        \n",
    "        x = tf.concat([x, y], 1)\n",
    "        a_out = self.actor(x)\n",
    "        c_out = self.critic(x)\n",
    "        return a_out, c_out, [m_state, c_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, act_spec, obs_spec):\n",
    "        self.h, self.w = 84, 84\n",
    "        self.gamma = 0.99\n",
    "        self.act_spec = act_spec\n",
    "        self.batch_size = 20\n",
    "        self.gym_steps = 1000\n",
    "        self.hidden_size = (256, 128)\n",
    "        \n",
    "        self.huber_loss = tf.keras.losses.Huber()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "        self.model = ActorCritic(act_spec, (self.h, self.w, 3), self.hidden_size)\n",
    "        \n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        \n",
    "    def save_weights(self, path):\n",
    "        self.model.save_weights(path)\n",
    "        \n",
    "    def compute_loss(self, actions, action_probs, values, returns):\n",
    "        advantage = np.array(returns) - tf.stop_gradient(values)\n",
    "        action_log_probs = tf.keras.losses.sparse_categorical_crossentropy(actions, action_probs)\n",
    "        actor_loss = tf.reduce_mean(-action_log_probs * advantage)\n",
    "        critic_loss = self.huber_loss(values, returns)\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def expected_return(self, rewards, stanardize=True):\n",
    "        n, returns, eps = len(rewards), [], np.finfo(np.float32).eps.item()\n",
    "        discount_sum = 0\n",
    "        for i in range(n):\n",
    "            reward = rewards[i]\n",
    "            discount_sum = reward + self.gamma * reward + self.gamma*self.gamma * discount_sum\n",
    "#             discount_sum = reward + self.gamma * discount_sum\n",
    "            returns.append(discount_sum)\n",
    "            \n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "        return returns\n",
    "    \n",
    "    def train_step(self, eps):\n",
    "        world, stage = 1, random.choice([1, 2, 3, 4])\n",
    "#         world, stage = 1, random.choice([1])\n",
    "        env = gym_super_mario_bros.make('SuperMarioBros-%d-%d-v0'%(world, stage))\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        state = env.reset()\n",
    "        _, _, done, preinfo = env.step(0)\n",
    "        model_state, action_history = None, [0 for _ in range(5)]\n",
    "        \n",
    "        ep_reward, ep_loss = 0, 0\n",
    "        for i in range(int(self.gym_steps/self.batch_size)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions, action_probs, values, rewards, next_data = run_episord(\n",
    "                    self.model, env, state, preinfo, done, eps, model_state, action_history,\n",
    "                    act_spec=self.act_spec, steps=self.batch_size, is_render=True)\n",
    "                returns = agent.expected_return(rewards)\n",
    "                actor_loss, critic_loss = self.compute_loss(actions, action_probs, values, returns)\n",
    "                loss = actor_loss + critic_loss\n",
    "                \n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            ep_loss += loss.numpy()\n",
    "            ep_reward += tf.math.reduce_sum(rewards).numpy()\n",
    "            \n",
    "            env, state, preinfo, done = next_data[0], next_data[1], next_data[2], next_data[3]\n",
    "            model_state, action_history = next_data[4], next_data[5]\n",
    "\n",
    "        env.close()\n",
    "        return ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episord(model, env, state, preinfo, done, eps, model_state, action_history, act_spec=7, steps=1000, is_render=False):\n",
    "    eps =  np.finfo(np.float32).eps.item()\n",
    "    actions, values, action_probs, rewards = [], [], [], []\n",
    "    seq_len = len(action_history)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            _, _, dones, preinfo = env.step(0)\n",
    "            if is_render: env.render()\n",
    "                \n",
    "        history = tf.one_hot([action_history], action_spec)\n",
    "        action_prob, value, model_state = model(state[np.newaxis, :, :, :], history, model_state)\n",
    "        action = np.random.choice([i for i in range(act_spec)], p=np.squeeze(action_prob))\n",
    "        current_state = state\n",
    "        action_history.append(action)\n",
    "        action_history = action_history[-seq_len:]\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward += (info['score'] - preinfo['score']) / 40.0\n",
    "        if done:\n",
    "            if info['flag_get']:\n",
    "                reward += 350.0\n",
    "            else:\n",
    "                reward -= 50.0\n",
    "        reward = reward / 10.0\n",
    "        \n",
    "        values.append(value)\n",
    "        actions.append(action)\n",
    "        action_probs.append(action_prob[0])\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        preinfo = info\n",
    "        if is_render: env.render()\n",
    "    return actions, action_probs, values, rewards, [env, state, preinfo, done, model_state, action_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "#0: #1:→ #2:→＋ジャンプ #3:→→\n",
    "#4:→→＋ジャンプ #5:ジャンプ #6:←\n",
    "action_spec = 7\n",
    "\n",
    "agent = Agent(action_spec, state.shape)\n",
    "agent.load_weights('./agents/checkpoint/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                               | 0/1000 [00:00<?, ?it/s]c:\\users\\mahoto\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:292: UserWarning: [WinError -2147417850] スレッド モードを設定してから変更することはできません。\n",
      "  warnings.warn(str(err))\n",
      "  4%|█████▉                                                                                                                                                           | 37/1000 [16:56<7:12:07, 26.92s/it, EpisordReward=147]"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "batch_size = 100\n",
    "size = 1000\n",
    "\n",
    "with tqdm(range(epochs)) as pbar:\n",
    "    for epoch in pbar:\n",
    "\n",
    "        eps = epoch / epochs\n",
    "        episord_reward = agent.train_step(eps)\n",
    "        pbar.set_postfix({'EpisordReward': episord_reward})\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            agent.save_weights('./agents/checkpoint/model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
